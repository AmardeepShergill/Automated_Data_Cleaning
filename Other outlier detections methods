# Other potenital motheds to detect outliers

In some cases the interquartile range may not be the mnost useful way of detecting outliers within a data set. This is cause by the interquartile range assuming that 1.5x the range is a threshold.
To counter this you have either use a different value for what you would consider and outliers or use one of the other methods mentioned below.


##  Z-Score Method

The Z-Score of the data point is used to show how far away from the mean a specific point is, this is then measured in standard deviations. In general any data that is more than 3 standard deviations above or below the mean would be flagged as an outlier. This value can again be changed assuming that you have some specific domain knowlege for the dataset. The main benifit of using this method is that it is simple to do and work espically well with normally distrivuted data, it does fall down when the data has a longer tail like in video games/book sales or is heavily skewed such as income distribution. 

Z-score is calculated using Z= (X- μ)σ where μ is the mean value of the data and σ is the standard deviation. The function below will calcuate the mean and standard deviationss of the column using numpy that use that to confirm the Z-Score. It then proceed to print any values that would be considered as outliers to the data.

def detect_outliers_zscore(data, threshold=3):
    # Calculate mean and standard deviation
    mean = np.mean(data)
    std = np.std(data)
    # Compute Z-scores
    z_scores = (data - mean) / std
    # Find indices where Z-scores exceed the threshold
    outlier_indices = np.where(np.abs(z_scores) > threshold)
    # Extract corresponding outlier values
    outlier_values = data[outlier_indices]
    return outlier_indices[0], outlier_values

indices, values = detect_outliers_zscore(data)  
print("Z-Score Outliers (Index, Value):")
for idx, val in zip(indices, values):
    print(f"Index: {idx}, Value: {val}")

## Local Outlier Factor
This is a unsupervide algorithm that will find outleir in relation to local neighbourhoods as opposed to using the entire data distribution. It then uses the denisity of nearby to show potential outliers that a seperated from the rest of the data where points with a low density of other local data will be marked. The function below can be used to help determine these outliers. This menthod worst best with complex data or datasets that have multiple dimensions, the difficulty is with this method is that it does require more fine tuning and wont work with data sets that do not have some inate density.   

from sklearn.neighbors import LocalOutlierFactor

def detect_outliers_lof(data, n_neighbors=20, contamination=0.1):
    """
    Detects outliers using the Local Outlier Factor (LOF) method.

    Args:
        data (numpy array): 1D array of numerical data.
        n_neighbors (int): Number of neighbors to consider for LOF.
        contamination (float): Proportion of the dataset assumed to be outliers.

    Returns:
        tuple: Indices and values of detected outliers.
    """
    # Reshape the 1D array for LOF (it expects a 2D array as input)
    reshaped_data = data.reshape(-1, 1)

    # Initialize and fit the LOF model
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)

    # Predict outliers: -1 for outliers, 1 for inliers
    outliers = lof.fit_predict(reshaped_data)

    # Get indices and values of outliers
    outlier_indices = np.where(outliers == -1)[0]
    outlier_values = data[outlier_indices]
    return outlier_indices, outlier_values

indices, values = detect_outliers_lof(data)  # Pass the 1D NumPy array
print("LOF Outliers (Index, Value):")
for idx, val in zip(indices, values):
    print(f"Index: {idx}, Value: {val}")

## Isolation Forest

Isolation Forest wroks by partition the data in a random order and isoloating individual points from there. In doing the any outliers end up being isoleted much faster than other points that are closer together. A contamination parameter is used to help set an expectation for the proportion of outliers in the data. 

This methods works especially with large datasets due to this, the main issue with this method is that results depend highly on the contamination parameter that is set in the code. Below is an example code that can be used to rund this method

def detect_outliers_isolation_forest(data, contamination=0.1):
    iso = IsolationForest(contamination=contamination, random_state=42)
    outliers = iso.fit_predict(data.reshape(-1, 1))
    outlier_indices = np.where(outliers == -1)[0]
    outlier_values = data[outlier_indices]
    return outlier_indices, outlier_values

indices, values = detect_outliers_isolation_forest(data)
print("Isolation Forest Outliers (Index, Value):")
for idx, val in zip(indices, values):
    print(f"Index: {idx}, Value: {val}")

